{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844338c4-bce1-4fef-be1e-afd349dcfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3571a5c-c23a-4601-8dcf-0f595d7d7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('v1.0-simplified_nq-dev-all.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "data = []\n",
    "for json_str in json_list:\n",
    "    data.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf749c3-9cde-46ac-a0c9-5c7e3701d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e7660-28da-4ed0-b9a0-12309807d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-05\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9bdf8-d410-4ef3-94f1-d4627555052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LongAnswerDataset(Dataset):\n",
    "    SAMPLE_RATE = 15\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_len=150, kaggle_format=True):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._kaggle_format = kaggle_format\n",
    "        \n",
    "        data = self._preprocess_data(data)\n",
    "        data = self._clean_df(data)\n",
    "        self._questions = data.question.values\n",
    "        self._long_answers = data.long_answer.values\n",
    "        self._targets = data.is_long_answer.values\n",
    "        \n",
    "        \n",
    "    def _get_nq_tokens(self, simplified_nq_example):\n",
    "        if \"document_text\" not in simplified_nq_example:\n",
    "            raise ValueError(\"`get_nq_tokens` should be called on a simplified NQ\"\n",
    "                         \"example that contains the `document_text` field.\")\n",
    "\n",
    "        return simplified_nq_example[\"document_text\"].split(\" \")\n",
    "    \n",
    "    def _clean_token(self, token):\n",
    "        return re.sub(u\" \", \"_\", token[\"token\"])\n",
    "\n",
    "    def _remove_html_byte_offsets(self, span):\n",
    "        if \"start_byte\" in span:\n",
    "            del span[\"start_byte\"]\n",
    "\n",
    "        if \"end_byte\" in span:\n",
    "            del span[\"end_byte\"]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def _clean_annotation(self, annotation):\n",
    "        annotation[\"long_answer\"] = self._remove_html_byte_offsets(\n",
    "            annotation[\"long_answer\"])\n",
    "        annotation[\"short_answers\"] = [\n",
    "            self._remove_html_byte_offsets(sa) for sa in annotation[\"short_answers\"]\n",
    "        ]\n",
    "        return annotation\n",
    "    \n",
    "    def _simplify_nq_example(self, nq_example):\n",
    "        text = \" \".join([self._clean_token(t) for t in nq_example[\"document_tokens\"]])\n",
    "\n",
    "        simplified_nq_example = {\n",
    "          \"question_text\": nq_example[\"question_text\"],\n",
    "          \"example_id\": nq_example[\"example_id\"],\n",
    "          \"document_url\": nq_example[\"document_url\"],\n",
    "          \"document_text\": text,\n",
    "          \"long_answer_candidates\": [\n",
    "              self._remove_html_byte_offsets(c)\n",
    "              for c in nq_example[\"long_answer_candidates\"]\n",
    "          ],\n",
    "          \"annotations\": [self._clean_annotation(a) for a in nq_example[\"annotations\"]]\n",
    "        }\n",
    "\n",
    "        if len(self._get_nq_tokens(simplified_nq_example)) != len(\n",
    "          nq_example[\"document_tokens\"]):\n",
    "            raise ValueError(\"Incorrect number of tokens.\")\n",
    "\n",
    "        return simplified_nq_example\n",
    "    \n",
    "    def _get_question_and_document(self, line):\n",
    "        question = line['question_text']\n",
    "        text = line['document_text'].split(' ')\n",
    "        annotations = line['annotations'][0]\n",
    "\n",
    "        return question, text, annotations\n",
    "\n",
    "\n",
    "    def _get_long_candidate(self, i, annotations, candidate):\n",
    "        if i == annotations['long_answer']['candidate_index']:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        # get place where long answer starts and ends in the document text\n",
    "        long_start = candidate['start_token']\n",
    "        long_end = candidate['end_token']\n",
    "\n",
    "        return label, long_start, long_end\n",
    "\n",
    "\n",
    "    def _form_data_row(self, question, label, text, long_start, long_end):\n",
    "        row = {\n",
    "            'question': question,\n",
    "            'long_answer': ' '.join(text[long_start:long_end]),\n",
    "            'is_long_answer': label,\n",
    "        }\n",
    "\n",
    "        return row\n",
    "\n",
    "\n",
    "    def _preprocess_data(self, data):\n",
    "        rows = []\n",
    "\n",
    "        for line in data:\n",
    "            if not self._kaggle_format:\n",
    "                line = self._simplify_nq_example(line)\n",
    "            question, text, annotations = self._get_question_and_document(line)\n",
    "            for i, candidate in enumerate(line['long_answer_candidates']):\n",
    "                label, long_start, long_end = self._get_long_candidate(i, annotations, candidate)\n",
    "\n",
    "                if label == True or (i % self.SAMPLE_RATE == 0):\n",
    "                    rows.append(\n",
    "                        self._form_data_row(question, label, text, long_start, long_end)\n",
    "                    )\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def _remove_stopwords(self, sentence):\n",
    "        words = sentence.split()\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def _remove_html(self, sentence):\n",
    "        html = re.compile(r'<.*?>')\n",
    "        return html.sub(r'', sentence)\n",
    "\n",
    "    def _clean_df_by_column(self, df, column):\n",
    "        df[column] = df[column].apply(lambda x : self._remove_stopwords(x))\n",
    "        df[column] = df[column].apply(lambda x : self._remove_html(x))\n",
    "        return df\n",
    "\n",
    "    def _clean_df(self, df):\n",
    "        df = self._clean_df_by_column(df, 'long_answer')\n",
    "        df = self._clean_df_by_column(df, 'question')\n",
    "        return df\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_tokens = self._questions[idx].split()\n",
    "        input_tokens.append(' ' + self._tokenizer.sep_token + ' ')\n",
    "        long_answer_tokens = self._long_answers[idx].split()\n",
    "        input_tokens.extend(long_answer_tokens)\n",
    "        encoding = self._tokenizer(input_tokens,\n",
    "                          is_split_into_words=True,\n",
    "                          return_offsets_mapping=False,\n",
    "                          return_token_type_ids=False,\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=self._max_len,\n",
    "                          return_tensors='pt')\n",
    "        return encoding, self._targets[idx]\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52814783-f43c-4236-aafa-f7e46c4ac599",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 312\n",
    "dataset = LongAnswerDataset(data[:7000], tokenizer, max_len, kaggle_format=False)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "dataset = LongAnswerDataset(data[7000:], tokenizer, max_len, kaggle_format=False)\n",
    "val_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55209cf4-de28-4ece-a69b-730d62cc41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba920d7f-ddb8-44d0-a36c-1db554cedb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongAnswerModel():\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device # or only in train?\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def plot_log(self, train_losses, val_losses, val_fscores):\n",
    "        clear_output()\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(7, 3))\n",
    "        fig.suptitle('Training Log', fontsize=8)\n",
    "        ax1.plot(train_losses)\n",
    "        ax1.set_title('Train Loss', fontsize=8)\n",
    "        ax1.tick_params(labelsize=6)\n",
    "        ax2.plot(val_losses)\n",
    "        ax2.set_title('Val Loss', fontsize=8)\n",
    "        ax2.tick_params(labelsize=6)\n",
    "        ax3.plot(val_fscores)\n",
    "        ax3.set_title(\"Val F1\", fontsize=8)\n",
    "        ax3.tick_params(labelsize=6)\n",
    "        plt.show()\n",
    "        \n",
    "    def validate(self, val_dataloader):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        val_loss, val_fscore = 0, 0\n",
    "            \n",
    "        for batch in val_dataloader:\n",
    "\n",
    "            tokens, labels = batch\n",
    "            ids = tokens['input_ids'].to(self.device).squeeze(dim=1)\n",
    "            mask = tokens['attention_mask'].to(self.device).squeeze(dim=1)\n",
    "            labels = labels.to(self.device) #.squeeze(dim=1)\n",
    "\n",
    "            output = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = output['loss']\n",
    "            logits = output['logits']\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            flattened_gold = labels.view(-1)\n",
    "            active_logits = logits.view(-1, model.num_labels)\n",
    "            flattened_pred = torch.argmax(active_logits, axis=1) \n",
    "\n",
    "            mask = labels.view(-1) != -100\n",
    "            gold = torch.masked_select(flattened_gold, mask)\n",
    "            pred = torch.masked_select(flattened_pred, mask)\n",
    "\n",
    "            fscore = f1_score(gold.cpu().numpy(), pred.cpu().numpy(), average='micro')\n",
    "            val_fscore += fscore\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_f1 = val_fscore / len(val_dataloader)\n",
    "        return avg_val_loss, avg_val_f1\n",
    "    \n",
    "    def train(self, train_dataloader, val_dataloader, n_epoch, optimizer):\n",
    "        \n",
    "        train_losses, val_losses = [], []\n",
    "        val_fscores = []\n",
    "                \n",
    "        for epoch in range(n_epoch):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            self.model.train()\n",
    "            \n",
    "            train_loss, train_fscore = 0, 0\n",
    "            \n",
    "            for batch in train_dataloader:\n",
    "                \n",
    "                tokens, labels = batch\n",
    "                ids = tokens['input_ids'].to(self.device).squeeze(dim=1)\n",
    "                mask = tokens['attention_mask'].to(self.device).squeeze(dim=1)\n",
    "                labels = labels.to(self.device) # .squeeze(dim=1)\n",
    "                \n",
    "                output = self.model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "                loss = output['loss']\n",
    "                logits = output['logits']\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # compute accuracy\n",
    "                flattened_gold = labels.view(-1)\n",
    "                active_logits = logits.view(-1, model.num_labels)\n",
    "                flattened_pred = torch.argmax(active_logits, axis=1) \n",
    "\n",
    "                mask = labels.view(-1) != -100\n",
    "                gold = torch.masked_select(flattened_gold, mask)\n",
    "                pred = torch.masked_select(flattened_pred, mask)\n",
    "\n",
    "                fscore = f1_score(gold.cpu().numpy(), pred.cpu().numpy(), average='micro')\n",
    "                train_fscore += fscore\n",
    "                \n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            avg_train_f1 = train_fscore / len(train_dataloader)\n",
    "            \n",
    "            avg_val_loss, avg_val_f1 = self.validate(val_dataloader)\n",
    "            \n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_fscores.append(avg_val_f1)\n",
    "            \n",
    "            self.plot_log(train_losses, val_losses, val_fscores)\n",
    "            \n",
    "            print(f'Epoch {epoch}')\n",
    "            print(f'Train loss: {avg_train_loss:.3f}')\n",
    "            print(f'Train micro F1: {avg_train_f1:.3f}')\n",
    "            print(f'Validation loss: {avg_val_loss:.3f}')\n",
    "            print(f'Validation micro F1: {avg_val_f1:.3f}')\n",
    "            curr_time = time.time() - start_time\n",
    "            print(f'Epoch time: {curr_time:.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96fd71-de97-4d32-9ec1-1a27e5c701f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_model = LongAnswerModel(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7b086-f3be-4666-9d64-e357ee7e82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_model.train(train_dataloader, val_dataloader, 3, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
