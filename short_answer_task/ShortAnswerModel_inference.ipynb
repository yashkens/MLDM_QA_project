{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5064b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e621bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_short_ans_dataset import ShortAnswerDataset, TestShortAnswerDataset, simplify_nq_example\n",
    "from short_ans_model import ShortAnswerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e51218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "        \n",
    "    data = []\n",
    "    for json_str in json_list:\n",
    "        data.append(json.loads(json_str))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = read_json(\"v1.0-simplified_nq-dev-all.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f97f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data = train_test_split(dev_data, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a131e7c",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb47faf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "col_spans = []\n",
    "for i in range(1, 10):\n",
    "    col_spans.append(f'<Td_colspan=\"{i}\">')\n",
    "    col_spans.append(f'<Th_colspan=\"{i}\">')\n",
    "tokenizer.add_tokens(['</Td>', '<Td>', '</Tr>', '<Tr>', '<Th>', '</Th>', '<Li>', '</Li>', '<Ul>', '</Ul>', '<Table>', '</Table>'])\n",
    "tokenizer.add_tokens(col_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ad8471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(29026, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f521df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3132/3132 [00:27<00:00, 112.35it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 500\n",
    "test_dataset = TestShortAnswerDataset(test_data, simplify_nq_example, tokenizer, max_len, should_simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3602561",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77525535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_test_example_preds(pred_starts, pred_ends):\n",
    "    if len(pred_starts) == pred_starts.count(-1):\n",
    "        return f\"{-1}:{-1}\"\n",
    "        \n",
    "    for i in range(len(pred_starts)):\n",
    "        if pred_starts[i] != -1:\n",
    "            return f\"{pred_starts[i]}:{pred_ends[i]}\"\n",
    "    return f\"{-1}:{-1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb5ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_gold(start, end, long_start):\n",
    "    if start == -1:\n",
    "        return f\"{-1}:{-1}\"\n",
    "    \n",
    "    start = start - long_start\n",
    "    end = end - long_start\n",
    "    return f\"{start}:{end}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b9dc635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('short_model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b353cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "answer_model = ShortAnswerModel(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e23a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1028/1028 [01:04<00:00, 15.82it/s]\n"
     ]
    }
   ],
   "source": [
    "pred, gold = [], []\n",
    "\n",
    "for data in tqdm(test_dataloader):\n",
    "    pred_starts, pred_ends = [], []\n",
    "    encodings, question_len, short_info, long_info = data\n",
    "    for i, enc in enumerate(encodings):\n",
    "        s, e = answer_model(enc, [question_len] * bs)[0]\n",
    "        s, e = int(s), int(e)\n",
    "        if s != -1:\n",
    "            s += (max_len-150)*i\n",
    "            e += (max_len-150)*i\n",
    "        pred_starts.append(s)\n",
    "        pred_ends.append(e)\n",
    "    pred_ans = join_test_example_preds(pred_starts, pred_ends)\n",
    "    gold_ans = format_test_gold(int(short_info[0]['start_token']), \n",
    "                                int(short_info[0]['end_token']), int(long_info[0]['start_token']))\n",
    "    pred.append(pred_ans)\n",
    "    gold.append(gold_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecee3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F score: 0.445\n"
     ]
    }
   ],
   "source": [
    "f = f1_score(gold, pred, average='micro')\n",
    "print(f'Test F score: {f:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
