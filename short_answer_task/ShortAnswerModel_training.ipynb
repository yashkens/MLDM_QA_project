{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf5a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938bd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_short_ans_dataset import ShortAnswerDataset, simplify_nq_example\n",
    "from short_ans_model import ShortAnswerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a3df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "        \n",
    "    data = []\n",
    "    for json_str in json_list:\n",
    "        data.append(json.loads(json_str))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b536a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_json(\"v1.0-simplified_simplified-nq-train.jsonl\")\n",
    "dev_data = read_json(\"v1.0-simplified_nq-dev-all.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06609d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data = train_test_split(dev_data, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d7f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 307373\n",
      "Validation size: 4698\n",
      "Test size: 3132\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405c853",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aaf347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87f8ab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_spans = []\n",
    "for i in range(1, 10):\n",
    "    col_spans.append(f'<Td_colspan=\"{i}\">')\n",
    "    col_spans.append(f'<Th_colspan=\"{i}\">')\n",
    "tokenizer.add_tokens(['</Td>', '<Td>', '</Tr>', '<Tr>', '<Th>', '</Th>', '<Li>', '</Li>', '<Ul>', '</Ul>', '<Table>', '</Table>'])\n",
    "tokenizer.add_tokens(col_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71b2976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(29026, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be73b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 307373/307373 [03:11<00:00, 1606.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4698/4698 [00:44<00:00, 105.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3132/3132 [00:30<00:00, 104.32it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 500\n",
    "\n",
    "train_dataset = ShortAnswerDataset(train_data, simplify_nq_example, tokenizer, max_len, should_simplify=False)\n",
    "val_dataset = ShortAnswerDataset(val_data, simplify_nq_example, tokenizer, max_len, should_simplify=True)\n",
    "test_dataset = ShortAnswerDataset(test_data, simplify_nq_example, tokenizer, max_len, should_simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec25652",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f863182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Steps: 5089\n",
      "Validation Steps: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Steps: {len(train_dataloader)}\")\n",
    "print(f\"Validation Steps: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ed29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-05\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a91845",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "answer_model = ShortAnswerModel(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db7ff48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_model.train(train_dataloader, val_dataloader, 5, optimizer, checkpoint_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "947b8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'short_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
